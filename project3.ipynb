{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/filipefborba/HandRecognition/blob/master/project3/project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sp4tlgEH60gM"
   },
   "source": [
    "# Hand Recognition\n",
    "### Filipe F. Borba  \n",
    "### Franklin W. Olin College of Engineering\n",
    "### Data Science, Prof. Allen Downey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbgSgBT8YwSp"
   },
   "source": [
    "Machine Learning is very useful for a variety of real-life problems. It is commonly used for tasks such as classification, recognition, detection and predictions. Moreover, it is very efficient to automate processes that use data. The basic idea is to use data to produce a model capable of returning an output. This output may give a right answer with a new input or produce predictions towards the known data.\n",
    "\n",
    "The goal of this project is to train a Machine Learning algorithm capable of classifying images of different hand gestures, such as a fist, palm, showing the thumb, and others. With this, I'll be able to understand more about this field and create my own program that fits the data that I have. This particular classification problem can be useful for [Gesture Navigation](https://www.youtube.com/watch?v=Lbma7c55wf8), for example. The method I'll be using is Deep Learning.\n",
    "\n",
    "Deep Learning is part of a broader family of machine learning methods. It is based on the use of layers that process the input data, extracting features from them and producing a mathematical model. The creation of this said 'model' will be more clear in the next session. In this specific project, we'll be aiming to classify different images of hand gestures, which means that the computer will have to \"learn\" the features of each gesture and classify them correctly. For example, if it is given an image of a hand doing a thumbs up gesture, the output of the model needs to be \"the hand is doing a thumbs up gesture\".\n",
    "\n",
    "Obs: This project was developed using the Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l20Rj1HP7LB-",
    "outputId": "9c81bcf6-a6a7-46a2-f49d-aef1d7648f80"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c19c845247e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# TensorFlow and tf.keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#from tensorflow import keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Here we import everything we need for the project\n",
    "\n",
    "%matplotlib inline\n",
    "#from google.colab import files\n",
    "import os\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split # Helps with organizing data for training\n",
    "from sklearn.metrics import confusion_matrix # Helps present results as a confusion-matrix\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXBc72Ph89nR"
   },
   "source": [
    "## Loading Data\n",
    "\n",
    "This project uses the [Hand Gesture Recognition Database](https://www.kaggle.com/gti-upm/leapgestrecog/version/1) (citation below) available on Kaggle. It contains 20000 images with different hands and hand gestures. There is a total of 10 hand gestures of 10 different people presented in the dataset. There are 5 female subjects and 5 male subjects.\n",
    "The images were captured using the Leap Motion hand tracking device.\n",
    "\n",
    ">Hand Gesture | Label used\n",
    ">--- | ---\n",
    "> Thumb down | 0\n",
    "> Palm (Horizontal) | 1\n",
    "> L | 2\n",
    "> Fist (Horizontal) | 3\n",
    "> Fist (Vertical) | 4\n",
    "> Thumbs up | 5\n",
    "> Index | 6\n",
    "> OK | 7\n",
    "> Palm (Vertical) | 8\n",
    "> C | 9\n",
    "\n",
    "Table 1 - Classification used for every hand gesture.\n",
    "\n",
    "\n",
    "T. Mantecón, C.R. del Blanco, F. Jaureguizar, N. García, “Hand Gesture Recognition using Infrared Imagery Provided by Leap Motion Controller”, Int. Conf. on Advanced Concepts for Intelligent Vision Systems, ACIVS 2016, Lecce, Italy, pp. 47-57, 24-27 Oct. 2016. (doi: 10.1007/978-3-319-48680-2_5)  \n",
    "\n",
    "Overview:\n",
    "- Load images\n",
    "- Some validation\n",
    "- Preparing the images for training\n",
    "- Use of train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pA0yTcvL_F_Z"
   },
   "outputs": [],
   "source": [
    "# Unzip images, ignore this cell if files are already in the workspace\n",
    "#!unzip leapGestRecog.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jRsNYIIoCmXp",
    "outputId": "4c51136b-9992-4064-876a-0c61730cdfa5"
   },
   "outputs": [],
   "source": [
    "# We need to get all the paths for the images to later load them\n",
    "imagepaths = []\n",
    "\n",
    "# Go through all the files and subdirectories inside a folder and save path to images inside list\n",
    "for root, dirs, files in os.walk(\"C:/Users/admin/Desktop/HandRecognition-master/leapGestRecog/\", topdown=False): \n",
    "  for name in files:\n",
    "    path = os.path.join(root, name)\n",
    "    if path.endswith(\"png\"): # We want only the images\n",
    "      imagepaths.append(path)\n",
    "\n",
    "print(len(imagepaths)) # If > 0, then a PNG image was loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ps9mMyvx_MQR"
   },
   "outputs": [],
   "source": [
    "# This function is used more for debugging and showing results later. It plots the image into the notebook\n",
    "\n",
    "def plot_image(path):\n",
    "  img = cv2.imread(path) # Reads the image into a numpy.array\n",
    "  img_cvt = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (RGB)\n",
    "  print(img_cvt.shape) # Prints the shape of the image just to check\n",
    "  plt.grid(False) # Without grid so we can see better\n",
    "  plt.imshow(img_cvt) # Shows the image\n",
    "  plt.xlabel(\"Width\")\n",
    "  plt.ylabel(\"Height\")\n",
    "  plt.title(\"Image \" + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "BJoQM_Yk__AX",
    "outputId": "ed463c9d-3c51-4c6d-ab24-e8cc080b2915"
   },
   "outputs": [],
   "source": [
    "plot_image(imagepaths[0]) #We plot the first image from our imagepaths array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mStukClHZm4p"
   },
   "source": [
    "Now that we loaded the images and checked if it's everything we expected, we have to prepare the images to train the algorithm. We have to load all the images into an array that we will call **X** and all the labels into another array called **y**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "LuQLrqJ2ZmOM",
    "outputId": "3f019f13-5250-408a-bc76-907f6947472f"
   },
   "outputs": [],
   "source": [
    "X = [] # Image data\n",
    "y = [] # Labels\n",
    "\n",
    "# Loops through imagepaths to load images and labels into arrays\n",
    "for path in imagepaths:\n",
    "  img = cv2.imread(path) # Reads image and returns np.array\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY)\n",
    "  img = cv2.resize(img, (320, 120)) # Reduce image size so training can be faster\n",
    "  X.append(img)\n",
    "  \n",
    "  # Processing label in image path\n",
    "  category = path.split(\"/\")[3]\n",
    "  label = int(category.split(\"_\")[0][1]) # We need to convert 10_down to 00_down, or else it crashes\n",
    "  y.append(label)\n",
    "\n",
    "# Turn X and y into np.array to speed up train_test_split\n",
    "X = np.array(X, dtype=\"uint8\")\n",
    "X = X.reshape(len(imagepaths), 120, 320, 1) # Needed to reshape so CNN knows it's different images\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Images loaded: \", len(X))\n",
    "print(\"Labels loaded: \", len(y))\n",
    "\n",
    "print(y[0], imagepaths[0]) # Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8AjUu9S6d11-"
   },
   "source": [
    "Scipy's train_test_split allows us to split our data into a training set and a test set. The training set will be used to build our model. Then, the test data will be used to check if our predictions are correct.  A random_state seed is used so the randomness of our results can be reproduced. The function will shuffle the images it's using to minimize training loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZa2YzKbdEz7"
   },
   "outputs": [],
   "source": [
    "ts = 0.3 # Percentage of images that we want to use for testing. The rest is used for training.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adafxZ8E8_ZB"
   },
   "source": [
    "## Creating Model\n",
    "\n",
    "To simplify the idea of the model being constructed here, we're going to use the concept of Linear Regression. By using linear regression, we can create a simple model and represent it using the equation ```y = ax + b```.   \n",
    "```a``` and ```b``` (slope and intercept, respectively) are the parameters that we're trying to find. By finding the best parameters, for any given value of x, we can predict y. This is the same idea here, but much more complex, with the use of Convolutional Neural Networks.\n",
    "\n",
    "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/0*hzIQ5Fs-g8iBpVWq.jpg\" alt=\"CNN Example\" width=\"400\">\n",
    "Figure 1 - Example of Convolutional Neural Network.\n",
    "\n",
    "From Figure 1 and imagining the Linear Regression model equation that we talked about, we can imagine that the input layer is x and the output layer is y. The hidden layers vary from model to model, but they are used to \"learn\" the parameters for our model. Each one has a different function, but they work towards getting the best \"slope and intercept\".\n",
    "\n",
    "\n",
    "Overview:\n",
    "- Import what the need\n",
    "- Creation of CNN\n",
    "- Compiling and training model\n",
    "- Saving model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzdh8xGTNQaj"
   },
   "outputs": [],
   "source": [
    "# Recreate the exact same model, including weights and optimizer.\n",
    "# model = keras.models.load_model('handrecognition_model.h5')\n",
    "# model.summary()\n",
    "\n",
    "# To use the pre-trained model, just load it and skip to the next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hq0ej5yDZq2e",
    "outputId": "e6e79eba-8386-4095-be6b-da102071a99b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2b2585c5378a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import of keras model and hidden layers for our convolutional network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Import of keras model and hidden layers for our convolutional network\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gq130kMucV41"
   },
   "source": [
    "Convolutional neural networks (CNNs) are the current state-of-the-art model architecture for image classification tasks. CNNs apply a series of filters to the raw pixel data of an image to extract and learn higher-level features, which the model can then use for classification. CNNs contains three components:\n",
    "\n",
    "- Convolutional layers, which apply a specified number of convolution filters to the image. For each subregion, the layer performs a set of mathematical operations to produce a single value in the output feature map. Convolutional layers then typically apply a ReLU activation function to the output to introduce nonlinearities into the model.\n",
    "\n",
    "- Pooling layers, which downsample the image data extracted by the convolutional layers to reduce the dimensionality of the feature map in order to decrease processing time. A commonly used pooling algorithm is max pooling, which extracts subregions of the feature map (e.g., 2x2-pixel tiles), keeps their maximum value, and discards all other values.\n",
    "\n",
    "- Dense (fully connected) layers, which perform classification on the features extracted by the convolutional layers and downsampled by the pooling layers. In a dense layer, every node in the layer is connected to every node in the preceding layer.\n",
    "\n",
    "https://www.tensorflow.org/tutorials/estimators/cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "iMiPmBWLYrkG",
    "outputId": "385d9b1d-aafb-4efa-f15c-22163b003ba0"
   },
   "outputs": [],
   "source": [
    "# Construction of model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 320, 1))) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YliBFXpgLZic"
   },
   "source": [
    "Our Convolutional Neural Network consists of different layers that have different functions. As explained before, the Conv2D layer performs a 2-D convolutional operation, which can be basically interpreted as a mathematical operation to calculate weights inside the image. In order to maximize the network's performance, we need to understand the parameters required by them.\n",
    "\n",
    "The first required by the Conv2D layer is the number of filters that the convolutional layer will learn. Layers early in the network architecture (closer to the actual input image) learn fewer convolutional filters while layers deeper in the network (closer to the output predictions) will learn more filters. This permits information to flow through the network without loss. These filters emulate edge detectors, blob detectors and other feature extraction methods.\n",
    "It is necessary to tune the values of the filters, but it is common practice to use powers of 2, starting with 32, 64, 128 and increasing to 256, 512, 1024, for example.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Ihab_S_Mohamed/publication/324165524/figure/fig3/AS:611103423860736@1522709818959/An-example-of-convolution-operation-in-2D-2.png\" alt=\"Example of 2D Convolution operation\" width=\"500\">\n",
    "\n",
    "Figure 2 - Example of 2D convolution operation.\n",
    "\n",
    "Another parameter required by the Conv2D layer is the kernel_size, a 2-tuple specifying the width and height of the 2D convolution window. The kernel_size must be an odd integer, with typical values of (1, 1) , (3, 3) , (5, 5) , (7, 7) . It’s rare to see kernel sizes larger than 7×7. If the input images are greater than 128×128 it is recommended to test a kernel size > 3 to help learn larger spatial filters and to help reduce volume size.\n",
    "\n",
    "Then, MaxPooling2D is used to reduce the spatial dimensions of the output volume. It reduces processing time and allows assumptions to be made about features contained in the sub-regions binned. It is possible to notice in this network that our output spatial volume is decreasing and our number of filters learned is increasing. This is a common practice in designing CNN architectures.\n",
    "\n",
    "Finally, ReLU stands for rectified linear unit, and is a type of activation function. ReLU is the most commonly used activation function in neural networks, especially in CNNs. ReLU is linear (identity) for all positive values, and zero for all negative values. This means that it's cheap to compute as there is no complicated math. The model can therefore take less time to train or run. Also, it converges faster by applying non-linearities to the model, so there is no 'vanishing gradient problem' suffered by other activation functions like sigmoid or tanh.\n",
    "\n",
    "In the end, there is a lot of trial and error to get the best parameters and network architecture. These are some common practices that help reach the best result faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amOpISRiYw3z"
   },
   "outputs": [],
   "source": [
    "# Configures the model for training\n",
    "model.compile(optimizer='adam', # Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function.\n",
    "              loss='sparse_categorical_crossentropy', # Loss function, which tells us how bad our predictions are.\n",
    "              metrics=['accuracy']) # List of metrics to be evaluated by the model during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "GMzJWqkGbkYl",
    "outputId": "cfae1b6a-e040-4b76-840c-c7cca1f3dabb"
   },
   "outputs": [],
   "source": [
    "# Trains the model for a given number of epochs (iterations on a dataset) and validates it.\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1zE7Dn4fJt0"
   },
   "outputs": [],
   "source": [
    "# Save entire model to a HDF5 file\n",
    "model.save('handrecognition_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrWvHfD09BUj"
   },
   "source": [
    "## Testing Model\n",
    "\n",
    "Now that we have the model compiled and trained, we need to check if it's good. First, we run ```model.evaluate``` to test the accuracy. Then, we make predictions and plot the images as long with the predicted labels and true labels to check everything. With that, we can see how our algorithm is working.  \n",
    "Later, we produce a confusion matrix, which is a specific table layout that allows visualization of the performance of an algorithm. \n",
    "\n",
    "Overview:\n",
    "- Evaluate model\n",
    "- Predictions\n",
    "- Plot images with predictions\n",
    "- Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "TjFW-Rx1lGaF",
    "outputId": "e26dbd0e-7f68-41ef-d9dc-6e403a824df2"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test accuracy: {:2.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8kgF7qFhahe"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test) # Make predictions towards the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-LGVByVLlx0Q",
    "outputId": "7047cbc6-3ca2-4153-8243-a718e04d0dcc"
   },
   "outputs": [],
   "source": [
    "np.argmax(predictions[0]), y_test[0] # If same, got it right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xurvEqibvZf"
   },
   "outputs": [],
   "source": [
    "# Function to plot images and labels for validation purposes\n",
    "def validate_9_images(predictions_array, true_label_array, img_array):\n",
    "  # Array for pretty printing and then figure size\n",
    "  class_names = [\"down\", \"palm\", \"l\", \"fist\", \"fist_moved\", \"thumb\", \"index\", \"ok\", \"palm_moved\", \"c\"] \n",
    "  plt.figure(figsize=(15,5))\n",
    "  \n",
    "  for i in range(1, 10):\n",
    "    # Just assigning variables\n",
    "    prediction = predictions_array[i]\n",
    "    true_label = true_label_array[i]\n",
    "    img = img_array[i]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Plot in a good way\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(prediction) # Get index of the predicted label from prediction\n",
    "    \n",
    "    # Change color of title based on good prediction or not\n",
    "    if predicted_label == true_label:\n",
    "      color = 'blue'\n",
    "    else:\n",
    "      color = 'red'\n",
    "\n",
    "    plt.xlabel(\"Predicted: {} {:2.0f}% (True: {})\".format(class_names[predicted_label],\n",
    "                                  100*np.max(prediction),\n",
    "                                  class_names[true_label]),\n",
    "                                  color=color)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "QSl0leiufKqF",
    "outputId": "f1909f59-4f16-4cb2-ce95-caf53c07c4bc"
   },
   "outputs": [],
   "source": [
    "validate_9_images(predictions, y_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJJlLR0hPYp1"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(predictions, axis=1) # Transform predictions into 1-D array with label number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "Tdza8t63PB7V",
    "outputId": "ab5c6770-09d9-40cd-c92c-142c41af6819"
   },
   "outputs": [],
   "source": [
    "# H = Horizontal\n",
    "# V = Vertical\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred), \n",
    "             columns=[\"Predicted Thumb Down\", \"Predicted Palm (H)\", \"Predicted L\", \"Predicted Fist (H)\", \"Predicted Fist (V)\", \"Predicted Thumbs up\", \"Predicted Index\", \"Predicted OK\", \"Predicted Palm (V)\", \"Predicted C\"],\n",
    "             index=[\"Actual Thumb Down\", \"Actual Palm (H)\", \"Actual L\", \"Actual Fist (H)\", \"Actual Fist (V)\", \"Actual Thumbs up\", \"Actual Index\", \"Actual OK\", \"Actual Palm (V)\", \"Actual C\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVd6-ROyUY6e"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Based on the results presented in the previous section, we can conclude that our algorithm successfully classifies different hand gestures images with enough confidence (>95%) based on a Deep Learning model.  \n",
    "\n",
    "The accuracy of our model is directly influenced by a few aspects of our problem. The gestures presented are reasonably distinct, the images are clear and without background. Also, there is a reasonable quantity of images, which makes our model more robust. The drawback is that for different problems, we would probably need more data to stir the parameters of our model into a better direction. Moreover, a deep learning model is very hard to interpret, given it's abstractions.  \n",
    "However, by using this approach it becomes much more easier to start working on the actual problem, since we don't have to account for feature engineering. This means that we don't need to pre-process the images with edge or blob detectors to extract the important features; the CNN does it for us. Also, it can be adapted to new problems relatively easily, with generally good performance.\n",
    "\n",
    "As mentioned, another approach to this problem would be to use feature engineering, such as binary thresholding (check area of the hand), circle detection and others to detect unique characteristics on the images. However, with our CNN approach, we don't have to worry about any of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TrqIdPIhknYZ"
   },
   "source": [
    "Any doubts? Feel free to send questions/issues on [the Github repository](https://github.com/filipefborba/HandRecognition)!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "HandRecognition.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
